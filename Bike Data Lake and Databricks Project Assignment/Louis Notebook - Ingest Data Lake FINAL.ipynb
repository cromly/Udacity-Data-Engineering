{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2991c70e-a906-4465-8acd-114bd3518acb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      " |-- _c2: string (nullable = true)\n",
      " |-- _c3: string (nullable = true)\n",
      "\n",
      "+---+----------+---+----+\n",
      "|_c0|       _c1|_c2| _c3|\n",
      "+---+----------+---+----+\n",
      "|  1|2019-05-01|9.0|1000|\n",
      "|  2|2019-06-01|9.0|1000|\n",
      "+---+----------+---+----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Create PySpark dataframes from the CSV file data from /FileStore/tables/udacity\n",
    "payments_df = spark.read.format(\"csv\") \\\n",
    "    .option(\"inferSchema\", \"false\") \\\n",
    "    .option(\"header\", \"false\") \\\n",
    "    .option(\"sep\", \",\") \\\n",
    ".load(\"/FileStore/tables/udacity/payments.csv\")\n",
    "\n",
    "riders_df = spark.read.format(\"csv\") \\\n",
    "    .option(\"inferSchema\", \"false\") \\\n",
    "    .option(\"header\", \"false\") \\\n",
    "    .option(\"sep\", \",\") \\\n",
    ".load(\"/FileStore/tables/udacity/riders.csv\")\n",
    "\n",
    "stations_df = spark.read.format(\"csv\") \\\n",
    "    .option(\"inferSchema\", \"false\") \\\n",
    "    .option(\"header\", \"false\") \\\n",
    "    .option(\"sep\", \",\") \\\n",
    ".load(\"/FileStore/tables/udacity/stations.csv\")\n",
    "\n",
    "trips_df = spark.read.format(\"csv\") \\\n",
    "    .option(\"inferSchema\", \"false\") \\\n",
    "    .option(\"header\", \"false\") \\\n",
    "    .option(\"sep\", \",\") \\\n",
    ".load(\"/FileStore/tables/udacity/trips.csv\")\n",
    "\n",
    "payments_df.printSchema()\n",
    "payments_df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "30c574ad-85b1-47a6-b037-1fa6e72a8b8f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Save the imported CSV file on DBFS into the Delta Lake space as a per default Parquet file\n",
    "payments_df.write.format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    ".save(\"/delta/bronze_payments\")\n",
    "\n",
    "riders_df.write.format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    ".save(\"/delta/bronze_riders\")\n",
    "\n",
    "stations_df.write.format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    ".save(\"/delta/bronze_stations\")\n",
    "\n",
    "trips_df.write.format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    ".save(\"/delta/bronze_trips\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6191c38-8585-4d08-88d7-c7197648290d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the BRONZE Delta Lake Table metadata for a database from the Parquet files in the Databricks File System\n",
    "spark.sql(\"CREATE TABLE IF NOT EXISTS bronze_payments_tbl USING DELTA LOCATION '/delta/bronze_payments'\")\n",
    "spark.sql(\"CREATE TABLE IF NOT EXISTS bronze_riders_tbl USING DELTA LOCATION '/delta/bronze_riders'\")\n",
    "spark.sql(\"CREATE TABLE IF NOT EXISTS bronze_stations_tbl USING DELTA LOCATION '/delta/bronze_stations'\")\n",
    "spark.sql(\"CREATE TABLE IF NOT EXISTS bronze_trips_tbl USING DELTA LOCATION '/delta/bronze_trips'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "293a237e-7d20-423e-b823-5a802ff51c89",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### OPTIONAL ####\n",
    "# DROP the SILVER tables if not wanted anymore\n",
    "spark.sql(\"DROP TABLE IF EXISTS silver_payments_tbl\")\n",
    "spark.sql(\"DROP TABLE IF EXISTS silver_riders_tbl\")\n",
    "spark.sql(\"DROP TABLE IF EXISTS silver_stations_tbl\")\n",
    "spark.sql(\"DROP TABLE IF EXISTS silver_trips_tbl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4298d770-d057-4f4b-b16f-e8b56e159851",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name='bronze_payments_tbl', catalog='spark_catalog', namespace=['default'], description=None, tableType='EXTERNAL', isTemporary=False),\n",
       " Table(name='bronze_riders_tbl', catalog='spark_catalog', namespace=['default'], description=None, tableType='EXTERNAL', isTemporary=False),\n",
       " Table(name='bronze_stations_tbl', catalog='spark_catalog', namespace=['default'], description=None, tableType='EXTERNAL', isTemporary=False),\n",
       " Table(name='bronze_trips_tbl', catalog='spark_catalog', namespace=['default'], description=None, tableType='EXTERNAL', isTemporary=False),\n",
       " Table(name='gold_station_dims_tbl', catalog='spark_catalog', namespace=['default'], description=None, tableType='MANAGED', isTemporary=False)]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List databases in the Catalog\n",
    "spark.catalog.listDatabases()\n",
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ca5b67d-ac85-40f7-ac85-181d378d8fa1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-------+\n",
      "|col_name|data_type|comment|\n",
      "+--------+---------+-------+\n",
      "|     _c0|   string|   NULL|\n",
      "|     _c1|   string|   NULL|\n",
      "|     _c2|   string|   NULL|\n",
      "|     _c3|   string|   NULL|\n",
      "+--------+---------+-------+\n",
      "\n",
      "+--------+---------+-------+\n",
      "|col_name|data_type|comment|\n",
      "+--------+---------+-------+\n",
      "|     _c0|   string|   NULL|\n",
      "|     _c1|   string|   NULL|\n",
      "|     _c2|   string|   NULL|\n",
      "|     _c3|   string|   NULL|\n",
      "|     _c4|   string|   NULL|\n",
      "|     _c5|   string|   NULL|\n",
      "|     _c6|   string|   NULL|\n",
      "|     _c7|   string|   NULL|\n",
      "+--------+---------+-------+\n",
      "\n",
      "+--------+---------+-------+\n",
      "|col_name|data_type|comment|\n",
      "+--------+---------+-------+\n",
      "|     _c0|   string|   NULL|\n",
      "|     _c1|   string|   NULL|\n",
      "|     _c2|   string|   NULL|\n",
      "|     _c3|   string|   NULL|\n",
      "+--------+---------+-------+\n",
      "\n",
      "+--------+---------+-------+\n",
      "|col_name|data_type|comment|\n",
      "+--------+---------+-------+\n",
      "|     _c0|   string|   NULL|\n",
      "|     _c1|   string|   NULL|\n",
      "|     _c2|   string|   NULL|\n",
      "|     _c3|   string|   NULL|\n",
      "|     _c4|   string|   NULL|\n",
      "|     _c5|   string|   NULL|\n",
      "|     _c6|   string|   NULL|\n",
      "+--------+---------+-------+\n",
      "\n",
      "+------+----------+---+-----+\n",
      "|   _c0|       _c1|_c2|  _c3|\n",
      "+------+----------+---+-----+\n",
      "|539256|2020-08-01|9.0|21826|\n",
      "|539257|2020-09-01|9.0|21826|\n",
      "|539258|2020-10-01|9.0|21826|\n",
      "+------+----------+---+-----+\n",
      "only showing top 3 rows\n",
      "\n",
      "+-----+-----+---------+--------------------+----------+----------+----+-----+\n",
      "|  _c0|  _c1|      _c2|                 _c3|       _c4|       _c5| _c6|  _c7|\n",
      "+-----+-----+---------+--------------------+----------+----------+----+-----+\n",
      "|57257| Mark|Mcfarland|   9928 Hunter Ranch|1982-02-01|2020-12-05|NULL|False|\n",
      "|57258| Mark|    Davis|20036 Barrett Sum...|1963-07-28|2017-07-12|NULL| True|\n",
      "|57259|Bryan|  Manning|    089 Sarah Square|1984-11-05|2018-08-10|NULL| True|\n",
      "+-----+-----+---------+--------------------+----------+----------+----+-----+\n",
      "only showing top 3 rows\n",
      "\n",
      "+------------+--------------------+-----------------+------------------+\n",
      "|         _c0|                 _c1|              _c2|               _c3|\n",
      "+------------+--------------------+-----------------+------------------+\n",
      "|         525|Glenwood Ave & To...|        42.012701|-87.66605799999999|\n",
      "|KA1503000012|  Clark St & Lake St|41.88579466666667|-87.63110066666668|\n",
      "|         637|Wood St & Chicago...|        41.895634|        -87.672069|\n",
      "+------------+--------------------+-----------------+------------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+----------------+-------------+-------------------+-------------------+------------+------------+-----+\n",
      "|             _c0|          _c1|                _c2|                _c3|         _c4|         _c5|  _c6|\n",
      "+----------------+-------------+-------------------+-------------------+------------+------------+-----+\n",
      "|7E1E50AC37E2DAD3| classic_bike|2021-08-14 14:01:36|2021-08-14 14:34:49|TA1309000007|       13089| 2644|\n",
      "|ADFF32195521E952| classic_bike|2021-08-29 16:16:36|2021-08-29 16:24:43|       13288|TA1308000031|37747|\n",
      "|7C59843DB8D13CC7|electric_bike|2021-08-27 11:06:34|2021-08-27 11:12:52|TA1307000062|TA1305000020|63224|\n",
      "+----------------+-------------+-------------------+-------------------+------------+------------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read and show Bronze tables SCHEMAs\n",
    "spark.sql(\"DESCRIBE TABLE bronze_payments_tbl\").show()\n",
    "spark.sql(\"DESCRIBE TABLE bronze_riders_tbl\").show()\n",
    "spark.sql(\"DESCRIBE TABLE bronze_stations_tbl\").show()\n",
    "spark.sql(\"DESCRIBE TABLE bronze_trips_tbl\").show()\n",
    "\n",
    "\n",
    "# Read and show bronze tables CONTECT\n",
    "spark.read.table(\"bronze_payments_tbl\").show(3)\n",
    "spark.read.table(\"bronze_riders_tbl\").show(3)\n",
    "spark.read.table(\"bronze_stations_tbl\").show(3)\n",
    "spark.read.table(\"bronze_trips_tbl\").show(3)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "503001eb-6829-43cb-bbc1-e62168014d75",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Exercise to develop a StructType Schema for databases, to make first column values not nulable. \n",
    "# Will be used with createDataFrame function below.\n",
    "# The PySpark DataTypes must be imported\n",
    "from pyspark.sql.types import * \n",
    "\n",
    "# Define the structures for the RDD dataframe Schema \n",
    "payments_schema = StructType([ \\\n",
    "    StructField('payment_id', IntegerType(), nullable=False), \\\n",
    "    StructField('date', DateType(), nullable=True), \\\n",
    "    StructField('amount', DecimalType(), nullable=True), \\\n",
    "    StructField('rider_id', IntegerType(), nullable=True) \\\n",
    "]) \n",
    "\n",
    "riders_schema = StructType([ \\\n",
    "    StructField('rider_id', IntegerType(), nullable=False), \\\n",
    "    StructField('first', StringType(), nullable=True), \\\n",
    "    StructField('last', StringType(), nullable=True), \\\n",
    "    StructField('address', StringType(), nullable=True), \\\n",
    "    StructField('birthday', DateType(), nullable=True), \\\n",
    "    StructField('account_start_date', DateType(), nullable=True), \\\n",
    "    StructField('account_end_date', DateType(), nullable=True), \\\n",
    "    StructField('is_member', BooleanType(), nullable=True) \\\n",
    "]) \n",
    "\n",
    "stations_schema = StructType([ \\\n",
    "    StructField('station_id', StringType(), nullable=False), \\\n",
    "    StructField('name', StringType(), nullable=True), \\\n",
    "    StructField('latitude', FloatType(), nullable=True), \\\n",
    "    StructField('longitude', FloatType(), nullable=True) \\\n",
    "]) \n",
    "\n",
    "trips_schema = StructType([ \\\n",
    "    StructField('trip_id', StringType(), nullable=False), \\\n",
    "    StructField('rideable_type', StringType(), nullable=True), \\\n",
    "    StructField('started_at', TimestampNTZType(), nullable=True), \\\n",
    "    StructField('ended_at', TimestampNTZType(), nullable=True), \\\n",
    "    StructField('start_station_id', IntegerType(), nullable=True), \\\n",
    "    StructField('end_station_id', IntegerType(), nullable=True), \\\n",
    "    StructField('rider_id', IntegerType(), nullable=True) \\\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a09422f3-cf7d-48eb-914f-774fd738482a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      " |-- _c2: string (nullable = true)\n",
      " |-- _c3: string (nullable = true)\n",
      "\n",
      "+----------+----------+------+--------+\n",
      "|payment_id|      date|amount|rider_id|\n",
      "+----------+----------+------+--------+\n",
      "|    539256|2020-08-01|     9|   21826|\n",
      "|    539257|2020-09-01|     9|   21826|\n",
      "|    539258|2020-10-01|     9|   21826|\n",
      "+----------+----------+------+--------+\n",
      "only showing top 3 rows\n",
      "\n",
      "root\n",
      " |-- payment_id: integer (nullable = true)\n",
      " |-- date: date (nullable = true)\n",
      " |-- amount: decimal(10,0) (nullable = true)\n",
      " |-- rider_id: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##### Create SILVER PySpark Dataframe #####\n",
    "#### silver_payments_df ####\n",
    "# Import required PySpark functions\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Create silver_payments_df PySpark dataframes with data from BRONZE Delta Lake Parquet Databricks File System\n",
    "silver_payments_df = spark.read.format(\"delta\").load(\"/delta/bronze_payments\")\n",
    "\n",
    "silver_payments_df.printSchema()\n",
    "\n",
    "# Rename and CAST Dataframe Columns\n",
    "silver_payments_df = silver_payments_df.withColumn(\"payment_id\",col(\"_c0\").cast(IntegerType()))\n",
    "silver_payments_df = silver_payments_df.withColumn(\"date\", col(\"_c1\").cast(DateType()))\n",
    "silver_payments_df = silver_payments_df.withColumn(\"amount\", col(\"_c2\").cast(DecimalType()))\n",
    "silver_payments_df = silver_payments_df.withColumn(\"rider_id\", col(\"_c3\").cast(IntegerType()))\n",
    "\n",
    "# Drop the original columns\n",
    "silver_payments_df = silver_payments_df.drop(\"_c0\")\n",
    "silver_payments_df = silver_payments_df.drop(\"_c1\")\n",
    "silver_payments_df = silver_payments_df.drop(\"_c2\")\n",
    "silver_payments_df = silver_payments_df.drop(\"_c3\")\n",
    "                                                   \n",
    "# Display Results\n",
    "silver_payments_df.show(3)\n",
    "silver_payments_df.printSchema()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10b8bb1a-46c7-41ad-86d0-cab852c5ef9a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+---------+--------------------+----------+------------------+----------------+---------+\n",
      "|rider_id|first|     last|             address|  birthday|account_start_date|account_end_date|is_member|\n",
      "+--------+-----+---------+--------------------+----------+------------------+----------------+---------+\n",
      "|   57257| Mark|Mcfarland|   9928 Hunter Ranch|1982-02-01|        2020-12-05|            NULL|    false|\n",
      "|   57258| Mark|    Davis|20036 Barrett Sum...|1963-07-28|        2017-07-12|            NULL|     true|\n",
      "|   57259|Bryan|  Manning|    089 Sarah Square|1984-11-05|        2018-08-10|            NULL|     true|\n",
      "+--------+-----+---------+--------------------+----------+------------------+----------------+---------+\n",
      "only showing top 3 rows\n",
      "\n",
      "root\n",
      " |-- rider_id: integer (nullable = true)\n",
      " |-- first: string (nullable = true)\n",
      " |-- last: string (nullable = true)\n",
      " |-- address: string (nullable = true)\n",
      " |-- birthday: date (nullable = true)\n",
      " |-- account_start_date: date (nullable = true)\n",
      " |-- account_end_date: date (nullable = true)\n",
      " |-- is_member: boolean (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##### Create SILVER PySpark Dataframe #####\n",
    "#### silver_riders_df ####\n",
    "# Import required PySpark functions\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Create silver_riders_df PySpark dataframes with data from Delta Lake Parquet Databricks File System\n",
    "silver_riders_df = spark.read.format(\"delta\").load(\"/delta/bronze_riders\")\n",
    "\n",
    "# Rename and CAST Dataframe Columns\n",
    "silver_riders_df = silver_riders_df.withColumn(\"rider_id\", col(\"_c0\").cast(IntegerType()))\n",
    "silver_riders_df = silver_riders_df.withColumn(\"first\", col(\"_c1\").cast(VarcharType(256)))\n",
    "silver_riders_df = silver_riders_df.withColumn(\"last\", col(\"_c2\").cast(VarcharType(256)))\n",
    "silver_riders_df = silver_riders_df.withColumn(\"address\", col(\"_c3\").cast(VarcharType(256)))\n",
    "silver_riders_df = silver_riders_df.withColumn(\"birthday\", col(\"_c4\").cast(DateType()))\n",
    "silver_riders_df = silver_riders_df.withColumn(\"account_start_date\", col(\"_c5\").cast(DateType()))\n",
    "silver_riders_df = silver_riders_df.withColumn(\"account_end_date\", col(\"_c6\").cast(DateType()))\n",
    "silver_riders_df = silver_riders_df.withColumn(\"is_member\", col(\"_c7\").cast(BooleanType()))\n",
    "\n",
    "# Drop the original columns\n",
    "silver_riders_df = silver_riders_df.drop(\"_c0\")\n",
    "silver_riders_df = silver_riders_df.drop(\"_c1\")\n",
    "silver_riders_df = silver_riders_df.drop(\"_c2\")\n",
    "silver_riders_df = silver_riders_df.drop(\"_c3\")\n",
    "silver_riders_df = silver_riders_df.drop(\"_c4\")\n",
    "silver_riders_df = silver_riders_df.drop(\"_c5\")\n",
    "silver_riders_df = silver_riders_df.drop(\"_c6\")\n",
    "silver_riders_df = silver_riders_df.drop(\"_c7\")\n",
    "\n",
    "# Display Results\n",
    "silver_riders_df.show(3)\n",
    "silver_riders_df.printSchema()\n",
    "\n",
    "\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bbf9bfe1-8675-4601-ac01-bf257293b90e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+---------+----------+\n",
      "|  station_id|                name| latitude| longitude|\n",
      "+------------+--------------------+---------+----------+\n",
      "|         525|Glenwood Ave & To...|  42.0127| -87.66606|\n",
      "|KA1503000012|  Clark St & Lake St|41.885796|  -87.6311|\n",
      "|         637|Wood St & Chicago...|41.895634|-87.672066|\n",
      "+------------+--------------------+---------+----------+\n",
      "only showing top 3 rows\n",
      "\n",
      "root\n",
      " |-- station_id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- latitude: float (nullable = true)\n",
      " |-- longitude: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##### Create SILVER PySpark Dataframe #####\n",
    "#### silver_stations_df ####\n",
    "# Import required PySpark functions\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Create silver_stations_df PySpark dataframes with data from Delta Lake Parquet Databricks File System\n",
    "silver_stations_df = spark.read.format(\"delta\").load(\"/delta/bronze_stations\")\n",
    "\n",
    "# Rename and CAST Dataframe Columns\n",
    "silver_stations_df = silver_stations_df.withColumn(\"station_id\", col(\"_c0\").cast(VarcharType(256)))\n",
    "silver_stations_df = silver_stations_df.withColumn(\"name\", col(\"_c1\").cast(VarcharType(256)))\n",
    "silver_stations_df = silver_stations_df.withColumn(\"latitude\", col(\"_c2\").cast(FloatType()))\n",
    "silver_stations_df = silver_stations_df.withColumn(\"longitude\", col(\"_c3\").cast(FloatType()))\n",
    "\n",
    "# Drop the original columns\n",
    "silver_stations_df = silver_stations_df.drop(\"_c0\")\n",
    "silver_stations_df = silver_stations_df.drop(\"_c1\")\n",
    "silver_stations_df = silver_stations_df.drop(\"_c2\")\n",
    "silver_stations_df = silver_stations_df.drop(\"_c3\")\n",
    "\n",
    "# Display Results\n",
    "silver_stations_df.show(3)\n",
    "silver_stations_df.printSchema()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a08aa85-2f1d-4bce-8d4b-e1b216ebdc1b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-------------+-------------------+-------------------+----------------+--------------+--------+\n",
      "|         trip_id|rideable_type|         started_at|           ended_at|start_station_id|end_station_id|rider_id|\n",
      "+----------------+-------------+-------------------+-------------------+----------------+--------------+--------+\n",
      "|7E1E50AC37E2DAD3| classic_bike|2021-08-14 14:01:36|2021-08-14 14:34:49|            NULL|         13089|    2644|\n",
      "|ADFF32195521E952| classic_bike|2021-08-29 16:16:36|2021-08-29 16:24:43|           13288|          NULL|   37747|\n",
      "|7C59843DB8D13CC7|electric_bike|2021-08-27 11:06:34|2021-08-27 11:12:52|            NULL|          NULL|   63224|\n",
      "+----------------+-------------+-------------------+-------------------+----------------+--------------+--------+\n",
      "only showing top 3 rows\n",
      "\n",
      "root\n",
      " |-- trip_id: string (nullable = true)\n",
      " |-- rideable_type: string (nullable = true)\n",
      " |-- started_at: timestamp_ntz (nullable = true)\n",
      " |-- ended_at: timestamp_ntz (nullable = true)\n",
      " |-- start_station_id: integer (nullable = true)\n",
      " |-- end_station_id: integer (nullable = true)\n",
      " |-- rider_id: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##### Create SILVER PySpark Dataframe #####\n",
    "#### silver_trips_df ####\n",
    "# Import required PySpark functions\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Create silver_stations_df PySpark dataframes with data from Delta Lake Parquet Databricks File System\n",
    "silver_trips_df = spark.read.format(\"delta\").load(\"/delta/bronze_trips\")\n",
    "\n",
    "# Rename and CAST Dataframe Columns\n",
    "silver_trips_df = silver_trips_df.withColumn(\"trip_id\", col(\"_c0\").cast(VarcharType(256)))\n",
    "silver_trips_df = silver_trips_df.withColumn(\"rideable_type\", col(\"_c1\").cast(StringType()))\n",
    "silver_trips_df = silver_trips_df.withColumn(\"started_at\", col(\"_c2\").cast(TimestampNTZType()))\n",
    "silver_trips_df = silver_trips_df.withColumn(\"ended_at\", col(\"_c3\").cast(TimestampNTZType()))\n",
    "silver_trips_df = silver_trips_df.withColumn(\"start_station_id\", col(\"_c4\").cast(IntegerType()))\n",
    "silver_trips_df = silver_trips_df.withColumn(\"end_station_id\", col(\"_c5\").cast(IntegerType()))\n",
    "silver_trips_df = silver_trips_df.withColumn(\"rider_id\", col(\"_c6\").cast(IntegerType()))\n",
    "\n",
    "# Drop the original columns\n",
    "silver_trips_df = silver_trips_df.drop(\"_c0\")\n",
    "silver_trips_df = silver_trips_df.drop(\"_c1\")\n",
    "silver_trips_df = silver_trips_df.drop(\"_c2\")\n",
    "silver_trips_df = silver_trips_df.drop(\"_c3\")\n",
    "silver_trips_df = silver_trips_df.drop(\"_c4\")\n",
    "silver_trips_df = silver_trips_df.drop(\"_c5\")\n",
    "silver_trips_df = silver_trips_df.drop(\"_c6\")\n",
    "\n",
    "# Display Results\n",
    "silver_trips_df.show(3)\n",
    "silver_trips_df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44ee9a83-55c8-4d9e-9c63-73856288147f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- payment_id: integer (nullable = false)\n",
      " |-- date: date (nullable = true)\n",
      " |-- amount: decimal(10,0) (nullable = true)\n",
      " |-- rider_id: integer (nullable = true)\n",
      "\n",
      "root\n",
      " |-- rider_id: integer (nullable = false)\n",
      " |-- first: string (nullable = true)\n",
      " |-- last: string (nullable = true)\n",
      " |-- address: string (nullable = true)\n",
      " |-- birthday: date (nullable = true)\n",
      " |-- account_start_date: date (nullable = true)\n",
      " |-- account_end_date: date (nullable = true)\n",
      " |-- is_member: boolean (nullable = true)\n",
      "\n",
      "root\n",
      " |-- station_id: string (nullable = false)\n",
      " |-- name: string (nullable = true)\n",
      " |-- latitude: float (nullable = true)\n",
      " |-- longitude: float (nullable = true)\n",
      "\n",
      "root\n",
      " |-- trip_id: string (nullable = false)\n",
      " |-- rideable_type: string (nullable = true)\n",
      " |-- started_at: timestamp_ntz (nullable = true)\n",
      " |-- ended_at: timestamp_ntz (nullable = true)\n",
      " |-- start_station_id: integer (nullable = true)\n",
      " |-- end_station_id: integer (nullable = true)\n",
      " |-- rider_id: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use createDataFrame to make first Column Values NOT NULLABLE\n",
    "silver_payments_df = spark.createDataFrame(silver_payments_df.rdd, payments_schema)\n",
    "silver_payments_df.printSchema()\n",
    "\n",
    "silver_riders_df = spark.createDataFrame(silver_riders_df.rdd, riders_schema)\n",
    "silver_riders_df.printSchema()\n",
    "\n",
    "silver_stations_df = spark.createDataFrame(silver_stations_df.rdd, stations_schema)\n",
    "silver_stations_df.printSchema()\n",
    "\n",
    "silver_trips_df = spark.createDataFrame(silver_trips_df.rdd, trips_schema)\n",
    "silver_trips_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19778726-c816-43af-81d1-34aacde586a9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- payment_id: integer (nullable = true)\n",
      " |-- date: date (nullable = true)\n",
      " |-- amount: decimal(10,0) (nullable = true)\n",
      " |-- rider_id: integer (nullable = true)\n",
      "\n",
      "+----------+----------+------+--------+\n",
      "|payment_id|      date|amount|rider_id|\n",
      "+----------+----------+------+--------+\n",
      "|   1574726|2021-02-01|     9|   61831|\n",
      "|   1574727|2021-03-01|     9|   61831|\n",
      "|   1574728|2021-04-01|     9|   61831|\n",
      "+----------+----------+------+--------+\n",
      "only showing top 3 rows\n",
      "\n",
      "root\n",
      " |-- rider_id: integer (nullable = true)\n",
      " |-- first: string (nullable = true)\n",
      " |-- last: string (nullable = true)\n",
      " |-- address: string (nullable = true)\n",
      " |-- birthday: date (nullable = true)\n",
      " |-- account_start_date: date (nullable = true)\n",
      " |-- account_end_date: date (nullable = true)\n",
      " |-- is_member: boolean (nullable = true)\n",
      "\n",
      "+--------+-----+---------+--------------------+----------+------------------+----------------+---------+\n",
      "|rider_id|first|     last|             address|  birthday|account_start_date|account_end_date|is_member|\n",
      "+--------+-----+---------+--------------------+----------+------------------+----------------+---------+\n",
      "|   57257| Mark|Mcfarland|   9928 Hunter Ranch|1982-02-01|        2020-12-05|            NULL|    false|\n",
      "|   57258| Mark|    Davis|20036 Barrett Sum...|1963-07-28|        2017-07-12|            NULL|     true|\n",
      "|   57259|Bryan|  Manning|    089 Sarah Square|1984-11-05|        2018-08-10|            NULL|     true|\n",
      "+--------+-----+---------+--------------------+----------+------------------+----------------+---------+\n",
      "only showing top 3 rows\n",
      "\n",
      "root\n",
      " |-- station_id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- latitude: float (nullable = true)\n",
      " |-- longitude: float (nullable = true)\n",
      "\n",
      "+------------+--------------------+---------+----------+\n",
      "|  station_id|                name| latitude| longitude|\n",
      "+------------+--------------------+---------+----------+\n",
      "|         525|Glenwood Ave & To...|  42.0127| -87.66606|\n",
      "|KA1503000012|  Clark St & Lake St|41.885796|  -87.6311|\n",
      "|         637|Wood St & Chicago...|41.895634|-87.672066|\n",
      "+------------+--------------------+---------+----------+\n",
      "only showing top 3 rows\n",
      "\n",
      "root\n",
      " |-- trip_id: string (nullable = true)\n",
      " |-- rideable_type: string (nullable = true)\n",
      " |-- started_at: timestamp_ntz (nullable = true)\n",
      " |-- ended_at: timestamp_ntz (nullable = true)\n",
      " |-- start_station_id: integer (nullable = true)\n",
      " |-- end_station_id: integer (nullable = true)\n",
      " |-- rider_id: integer (nullable = true)\n",
      "\n",
      "+----------------+-------------+-------------------+-------------------+----------------+--------------+--------+\n",
      "|         trip_id|rideable_type|         started_at|           ended_at|start_station_id|end_station_id|rider_id|\n",
      "+----------------+-------------+-------------------+-------------------+----------------+--------------+--------+\n",
      "|89E7AA6C29227EFF| classic_bike|2021-02-12 16:14:56|2021-02-12 16:21:43|             525|           660|   71934|\n",
      "|0FEFDE2603568365| classic_bike|2021-02-14 17:52:38|2021-02-14 18:12:09|             525|         16806|   47854|\n",
      "|E6159D746B2DBB91|electric_bike|2021-02-09 19:10:18|2021-02-09 19:19:10|            NULL|          NULL|   70870|\n",
      "+----------------+-------------+-------------------+-------------------+----------------+--------------+--------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##### Create SILVER Tables #####\n",
    "# Create the SILVER tables in the Delta Lake Lakehouse Structure from dataframes\n",
    "silver_payments_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"silver_payments_tbl\")\n",
    "spark.read.table(\"silver_payments_tbl\").printSchema()\n",
    "spark.read.table(\"silver_payments_tbl\").show(3)\n",
    "\n",
    "silver_riders_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"silver_riders_tbl\")\n",
    "spark.read.table(\"silver_riders_tbl\").printSchema()\n",
    "spark.read.table(\"silver_riders_tbl\").show(3)\n",
    "\n",
    "silver_stations_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"silver_stations_tbl\")\n",
    "spark.read.table(\"silver_stations_tbl\").printSchema()\n",
    "spark.read.table(\"silver_stations_tbl\").show(3)\n",
    "\n",
    "silver_trips_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"silver_trips_tbl\")\n",
    "spark.read.table(\"silver_trips_tbl\").printSchema()\n",
    "spark.read.table(\"silver_trips_tbl\").show(3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5300e60b-4fc6-4f1c-bfbe-32c70ad3591a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name='bronze_payments_tbl', catalog='spark_catalog', namespace=['default'], description=None, tableType='EXTERNAL', isTemporary=False),\n",
       " Table(name='bronze_riders_tbl', catalog='spark_catalog', namespace=['default'], description=None, tableType='EXTERNAL', isTemporary=False),\n",
       " Table(name='bronze_stations_tbl', catalog='spark_catalog', namespace=['default'], description=None, tableType='EXTERNAL', isTemporary=False),\n",
       " Table(name='bronze_trips_tbl', catalog='spark_catalog', namespace=['default'], description=None, tableType='EXTERNAL', isTemporary=False),\n",
       " Table(name='gold_pament_facts_tbl', catalog='spark_catalog', namespace=['default'], description=None, tableType='MANAGED', isTemporary=False),\n",
       " Table(name='gold_rider_dims_tbl', catalog='spark_catalog', namespace=['default'], description=None, tableType='MANAGED', isTemporary=False),\n",
       " Table(name='gold_station_dims_tbl', catalog='spark_catalog', namespace=['default'], description=None, tableType='MANAGED', isTemporary=False),\n",
       " Table(name='gold_trip_facts_tbl', catalog='spark_catalog', namespace=['default'], description=None, tableType='MANAGED', isTemporary=False),\n",
       " Table(name='silver_payments_tbl', catalog='spark_catalog', namespace=['default'], description=None, tableType='MANAGED', isTemporary=False),\n",
       " Table(name='silver_riders_tbl', catalog='spark_catalog', namespace=['default'], description=None, tableType='MANAGED', isTemporary=False),\n",
       " Table(name='silver_stations_tbl', catalog='spark_catalog', namespace=['default'], description=None, tableType='MANAGED', isTemporary=False),\n",
       " Table(name='silver_trips_tbl', catalog='spark_catalog', namespace=['default'], description=None, tableType='MANAGED', isTemporary=False)]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List databases in the Catalog\n",
    "spark.catalog.listDatabases()\n",
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4bfc5cd5-44be-4162-9c0e-7f21e67c3ff7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- date_id: timestamp_ntz (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- quarter: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- week: integer (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- is_weekend: boolean (nullable = false)\n",
      " |-- hour: integer (nullable = true)\n",
      "\n",
      "+-------------------+----+-------+-----+----+---+----------+----+\n",
      "|            date_id|year|quarter|month|week|day|is_weekend|hour|\n",
      "+-------------------+----+-------+-----+----+---+----------+----+\n",
      "|2021-02-10 17:44:28|2021|      1|    2|   6| 10|     false|  17|\n",
      "|2021-02-28 21:09:14|2021|      1|    2|   8| 28|     false|  21|\n",
      "|2021-02-28 13:56:01|2021|      1|    2|   8| 28|     false|  13|\n",
      "|2021-02-05 10:45:16|2021|      1|    2|   5|  5|      true|  10|\n",
      "|2021-02-03 16:32:44|2021|      1|    2|   5|  3|     false|  16|\n",
      "+-------------------+----+-------+-----+----+---+----------+----+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- date_id: timestamp_ntz (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- quarter: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- week: integer (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- is_weekend: boolean (nullable = true)\n",
      " |-- hour: integer (nullable = true)\n",
      "\n",
      "+-------------------+----+-------+-----+----+---+----------+----+\n",
      "|            date_id|year|quarter|month|week|day|is_weekend|hour|\n",
      "+-------------------+----+-------+-----+----+---+----------+----+\n",
      "|2021-02-02 17:27:52|2021|      1|    2|   5|  2|     false|  17|\n",
      "|2021-02-09 06:54:18|2021|      1|    2|   6|  9|     false|   6|\n",
      "|2021-02-25 12:52:04|2021|      1|    2|   8| 25|     false|  12|\n",
      "+-------------------+----+-------+-----+----+---+----------+----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##### Create GOLD Dimension Dataframe #####\n",
    "# Building Gold date_dims table\n",
    "\n",
    "gold_date_dims_df = spark.sql(\"\"\"\n",
    "SELECT\n",
    "    DISTINCT t.started_at AS date_id,\n",
    "    EXTRACT(YEAR FROM t.started_at) AS year,\n",
    "\tEXTRACT(QUARTER FROM t.started_at) AS quarter,\n",
    "\tEXTRACT(MONTH FROM t.started_at) AS month,\n",
    "\tEXTRACT(WEEK FROM t.started_at) AS week,\n",
    "\tEXTRACT(DAY FROM t.started_at) AS day,\n",
    "\tCASE WHEN dayofweek(t.started_at) IN (6, 7) THEN true ELSE false END AS is_weekend,\n",
    "\tEXTRACT(HOUR FROM t.started_at) AS hour\n",
    "FROM default.silver_trips_tbl AS t\n",
    "\n",
    "UNION\n",
    "\n",
    "SELECT \n",
    "    DISTINCT t.ended_at AS date_id,\n",
    "    EXTRACT(YEAR FROM t.ended_at) AS year,\n",
    "\tEXTRACT(QUARTER FROM t.ended_at) AS quarter,\n",
    "\tEXTRACT(MONTH FROM t.ended_at) AS month,\n",
    "\tEXTRACT(WEEK FROM t.ended_at) AS week,\n",
    "\tEXTRACT(DAY FROM t.ended_at) AS day,\n",
    "\tCASE WHEN dayofweek(t.ended_at) IN (6, 7) THEN true ELSE false END AS is_weekend,\n",
    "\tEXTRACT(HOUR FROM t.ended_at) AS hour\n",
    "FROM default.silver_trips_tbl AS t\n",
    "\n",
    "UNION\n",
    "\n",
    "SELECT \n",
    "    DISTINCT p.date AS date_id,\n",
    "    EXTRACT(YEAR FROM p.date) AS year,\n",
    "\tEXTRACT(QUARTER FROM p.date) AS quarter,\n",
    "\tEXTRACT(MONTH FROM p.date) AS month,\n",
    "\tEXTRACT(WEEK FROM p.date) AS week,\n",
    "\tEXTRACT(DAY FROM p.date) AS day,\n",
    "\tCASE WHEN dayofweek(p.date) IN (6, 7) THEN true ELSE false END AS is_weekend,\n",
    "\tNULL AS hour\n",
    "FROM default.silver_payments_tbl AS p;\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "gold_date_dims_df.printSchema()\n",
    "gold_date_dims_df.show(5)\n",
    "\n",
    "##### Create GOLD Dimension Table #####                   \n",
    "gold_date_dims_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"gold_date_dims_tbl\")\n",
    "\n",
    "spark.read.table(\"gold_date_dims_tbl\").printSchema()\n",
    "spark.read.table(\"gold_date_dims_tbl\").show(3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52c558b0-5520-4a29-ba88-7a77784f9840",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- station_id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n",
      "+------------+--------------------+\n",
      "|  station_id|                name|\n",
      "+------------+--------------------+\n",
      "|         525|Glenwood Ave & To...|\n",
      "|KA1503000012|  Clark St & Lake St|\n",
      "|         637|Wood St & Chicago...|\n",
      "|       13216|  State St & 33rd St|\n",
      "|       18003|Fairbanks St & Su...|\n",
      "+------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- station_id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n",
      "+------------+--------------------+\n",
      "|  station_id|                name|\n",
      "+------------+--------------------+\n",
      "|         525|Glenwood Ave & To...|\n",
      "|KA1503000012|  Clark St & Lake St|\n",
      "|         637|Wood St & Chicago...|\n",
      "+------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##### Create GOLD Dimension Dataframe #####\n",
    "# Building Gold station_dims table\n",
    "\n",
    "gold_station_dims_df = spark.sql(\"\"\"SELECT s.station_id, s.name \n",
    "FROM default.silver_stations_tbl AS s\"\"\")\n",
    "\n",
    "gold_station_dims_df.printSchema()\n",
    "gold_station_dims_df.show(5)\n",
    "\n",
    "##### Create GOLD Dimension Table #####\n",
    "gold_station_dims_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"default.gold_station_dims_tbl\")\n",
    "\n",
    "spark.read.table(\"gold_station_dims_tbl\").printSchema()\n",
    "spark.read.table(\"gold_station_dims_tbl\").show(3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1437664c-406e-474f-86fa-03c755eb8dc9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- rider_id: integer (nullable = true)\n",
      " |-- first: string (nullable = true)\n",
      " |-- last: string (nullable = true)\n",
      " |-- address: string (nullable = true)\n",
      " |-- birthday: date (nullable = true)\n",
      " |-- is_member: boolean (nullable = true)\n",
      " |-- account_start_date: date (nullable = true)\n",
      " |-- account_end_date: date (nullable = true)\n",
      " |-- age_at_acc_start: long (nullable = true)\n",
      "\n",
      "+--------+-----------+---------+--------------------+----------+---------+------------------+----------------+----------------+\n",
      "|rider_id|      first|     last|             address|  birthday|is_member|account_start_date|account_end_date|age_at_acc_start|\n",
      "+--------+-----------+---------+--------------------+----------+---------+------------------+----------------+----------------+\n",
      "|    1000|      Diana|    Clark| 1200 Alyssa Squares|1989-02-13|     true|        2019-04-23|            NULL|             -30|\n",
      "|    1001|   Jennifer|    Smith|     397 Diana Ferry|1976-08-10|     true|        2019-11-01|      2020-09-01|             -43|\n",
      "|    1002|      Karen|    Smith|644 Brittany Row ...|1998-08-10|     true|        2022-02-04|            NULL|             -23|\n",
      "|    1003|      Bryan|  Roberts|996 Dickerson Tur...|1999-03-29|    false|        2019-08-26|            NULL|             -20|\n",
      "|    1004|      Jesse|Middleton|7009 Nathan Expre...|1969-04-11|     true|        2019-09-14|            NULL|             -50|\n",
      "|    1005|  Christine|Rodriguez|224 Washington Mi...|1974-08-27|    false|        2020-03-24|            NULL|             -45|\n",
      "|    1006|     Alicia|   Taylor|   1137 Angela Locks|2004-01-30|     true|        2020-11-27|      2021-12-01|             -16|\n",
      "|    1007|   Benjamin|Fernandez|   979 Phillips Ways|1988-01-11|    false|        2016-12-11|            NULL|             -28|\n",
      "|    1008|       John| Crawford|    7691 Evans Court|1987-02-21|     true|        2021-03-28|      2021-07-01|             -34|\n",
      "|    1009|   Victoria|   Ritter|9922 Jim Crest Ap...|1981-02-07|     true|        2020-06-12|      2021-11-01|             -39|\n",
      "|    1010|      Tracy|   Austin|    92973 Mary Ville|1996-04-07|     true|        2019-12-27|            NULL|             -23|\n",
      "|    1011|    Jessica|    Mcgee|950 Grimes Burg A...|1984-12-29|     true|        2017-05-20|            NULL|             -32|\n",
      "|    1012|    Heather|   Fisher|65532 Davis Sprin...|1980-10-20|     true|        2021-10-16|            NULL|             -40|\n",
      "|    1013|    Timothy|    Jones| 7757 Johnston Roads|1985-07-10|     true|        2020-12-28|      2021-11-01|             -35|\n",
      "|    1014|   Jennifer|   Martin|   501 Arellano Land|1989-12-04|     true|        2017-11-24|            NULL|             -27|\n",
      "|    1015|Christopher|    Silva|3710 Rodriguez Gl...|2001-07-25|     true|        2017-07-10|            NULL|             -15|\n",
      "|    1016|     Andrew|    Jones|  72226 Casey Square|1991-12-13|     true|        2022-02-02|            NULL|             -30|\n",
      "|    1017|    William|   Lawson| 40395 Terrell Parks|1981-04-17|     true|        2019-03-11|            NULL|             -37|\n",
      "|    1018|     Shelly|   Briggs|   3514 Leslie Vista|1986-09-02|    false|        2021-07-25|            NULL|             -34|\n",
      "|    1019|       Tina|   Garcia|00348 Brandi Park...|1997-05-03|    false|        2021-07-10|      2022-01-01|             -24|\n",
      "+--------+-----------+---------+--------------------+----------+---------+------------------+----------------+----------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- rider_id: integer (nullable = true)\n",
      " |-- first: string (nullable = true)\n",
      " |-- last: string (nullable = true)\n",
      " |-- address: string (nullable = true)\n",
      " |-- birthday: date (nullable = true)\n",
      " |-- is_member: boolean (nullable = true)\n",
      " |-- account_start_date: date (nullable = true)\n",
      " |-- account_end_date: date (nullable = true)\n",
      " |-- age_at_acc_start: long (nullable = true)\n",
      "\n",
      "+--------+-----+---------+--------------------+----------+---------+------------------+----------------+----------------+\n",
      "|rider_id|first|     last|             address|  birthday|is_member|account_start_date|account_end_date|age_at_acc_start|\n",
      "+--------+-----+---------+--------------------+----------+---------+------------------+----------------+----------------+\n",
      "|   57257| Mark|Mcfarland|   9928 Hunter Ranch|1982-02-01|    false|        2020-12-05|            NULL|             -38|\n",
      "|   57258| Mark|    Davis|20036 Barrett Sum...|1963-07-28|     true|        2017-07-12|            NULL|             -53|\n",
      "|   57259|Bryan|  Manning|    089 Sarah Square|1984-11-05|     true|        2018-08-10|            NULL|             -33|\n",
      "+--------+-----+---------+--------------------+----------+---------+------------------+----------------+----------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##### Create GOLD Dimension Dataframe #####\n",
    "# Building Gold rider_dims table\n",
    "\n",
    "gold_rider_dims_df = spark.sql(\"\"\"\n",
    "SELECT \n",
    "r.rider_id, r.first, r.last, r.address, r.birthday, r.is_member, r.account_start_date, r.account_end_date, \n",
    "DATEDIFF(year, r.account_start_date, r.birthday) AS age_at_acc_start\n",
    "FROM default.silver_riders_tbl AS r\n",
    "\"\"\")\n",
    "\n",
    "gold_rider_dims_df.printSchema()\n",
    "gold_rider_dims_df.show()\n",
    "\n",
    "##### Create GOLD Dimension Table #####\n",
    "gold_rider_dims_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"default.gold_rider_dims_tbl\")\n",
    "\n",
    "spark.read.table(\"gold_rider_dims_tbl\").printSchema()\n",
    "spark.read.table(\"gold_rider_dims_tbl\").show(3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "653e1653-6b90-45a2-b4ad-6bc67d87b019",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- trip_id: string (nullable = true)\n",
      " |-- started_at_date_id: timestamp_ntz (nullable = true)\n",
      " |-- ended_at_date_id: timestamp_ntz (nullable = true)\n",
      " |-- start_station_id: integer (nullable = true)\n",
      " |-- end_station_id: integer (nullable = true)\n",
      " |-- rider_id: integer (nullable = true)\n",
      " |-- rider_age: long (nullable = true)\n",
      " |-- trip_duration: long (nullable = true)\n",
      "\n",
      "+----------------+-------------------+-------------------+----------------+--------------+--------+---------+-------------+\n",
      "|         trip_id| started_at_date_id|   ended_at_date_id|start_station_id|end_station_id|rider_id|rider_age|trip_duration|\n",
      "+----------------+-------------------+-------------------+----------------+--------------+--------+---------+-------------+\n",
      "|89E7AA6C29227EFF|2021-02-12 16:14:56|2021-02-12 16:21:43|             525|           660|   71934|      -37|            0|\n",
      "|0FEFDE2603568365|2021-02-14 17:52:38|2021-02-14 18:12:09|             525|         16806|   47854|      -38|            0|\n",
      "|E6159D746B2DBB91|2021-02-09 19:10:18|2021-02-09 19:19:10|            NULL|          NULL|   70870|      -33|            0|\n",
      "|B32D3199F1C2E75B|2021-02-02 17:49:41|2021-02-02 17:54:06|             637|          NULL|   58974|      -19|            0|\n",
      "|83E463F23575F4BF|2021-02-23 15:07:23|2021-02-23 15:22:37|           13216|          NULL|   39608|      -71|            0|\n",
      "+----------------+-------------------+-------------------+----------------+--------------+--------+---------+-------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- trip_id: string (nullable = true)\n",
      " |-- started_at_date_id: timestamp_ntz (nullable = true)\n",
      " |-- ended_at_date_id: timestamp_ntz (nullable = true)\n",
      " |-- start_station_id: integer (nullable = true)\n",
      " |-- end_station_id: integer (nullable = true)\n",
      " |-- rider_id: integer (nullable = true)\n",
      " |-- rider_age: long (nullable = true)\n",
      " |-- trip_duration: long (nullable = true)\n",
      "\n",
      "+----------------+-------------------+-------------------+----------------+--------------+--------+---------+-------------+\n",
      "|         trip_id| started_at_date_id|   ended_at_date_id|start_station_id|end_station_id|rider_id|rider_age|trip_duration|\n",
      "+----------------+-------------------+-------------------+----------------+--------------+--------+---------+-------------+\n",
      "|89E7AA6C29227EFF|2021-02-12 16:14:56|2021-02-12 16:21:43|             525|           660|   71934|      -37|            0|\n",
      "|0FEFDE2603568365|2021-02-14 17:52:38|2021-02-14 18:12:09|             525|         16806|   47854|      -38|            0|\n",
      "|E6159D746B2DBB91|2021-02-09 19:10:18|2021-02-09 19:19:10|            NULL|          NULL|   70870|      -33|            0|\n",
      "+----------------+-------------------+-------------------+----------------+--------------+--------+---------+-------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##### Create GOLD Dimension Dataframe #####\n",
    "# Building Gold trip_facts table\n",
    "\n",
    "gold_trip_facts_df = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "    t.trip_id,\n",
    "\tt.started_at AS started_at_date_id,\n",
    "\tt.ended_at AS ended_at_date_id,\n",
    "\tt.start_station_id,\n",
    "\tt.end_station_id, \n",
    "\tt.rider_id,\n",
    "\tDATEDIFF(YEAR, t.started_at, r.birthday) AS rider_age, \n",
    "\tDATEDIFF(HOUR, t.ended_at, t.started_at) AS trip_duration\n",
    "FROM default.silver_trips_tbl AS t \n",
    "JOIN default.silver_riders_tbl AS r ON t.rider_id = r.rider_id;\n",
    "\"\"\")\n",
    "\n",
    "gold_trip_facts_df.printSchema()\n",
    "gold_trip_facts_df.show(5)\n",
    "\n",
    "##### Create GOLD Dimension Table #####\n",
    "gold_trip_facts_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"default.gold_trip_facts_tbl\")\n",
    "\n",
    "spark.read.table(\"gold_trip_facts_tbl\").printSchema()\n",
    "spark.read.table(\"gold_trip_facts_tbl\").show(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "82708483-e7f1-4cff-af0a-58332ecc3d99",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- payment_id: integer (nullable = true)\n",
      " |-- date_id: date (nullable = true)\n",
      " |-- rider_id: integer (nullable = true)\n",
      " |-- pay_amount: decimal(10,0) (nullable = true)\n",
      "\n",
      "+----------+----------+--------+----------+\n",
      "|payment_id|   date_id|rider_id|pay_amount|\n",
      "+----------+----------+--------+----------+\n",
      "|   1574726|2021-02-01|   61831|         9|\n",
      "|   1574727|2021-03-01|   61831|         9|\n",
      "|   1574728|2021-04-01|   61831|         9|\n",
      "|   1574729|2021-05-01|   61831|         9|\n",
      "|   1574730|2021-06-01|   61831|         9|\n",
      "+----------+----------+--------+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- payment_id: integer (nullable = true)\n",
      " |-- date_id: date (nullable = true)\n",
      " |-- rider_id: integer (nullable = true)\n",
      " |-- pay_amount: decimal(10,0) (nullable = true)\n",
      "\n",
      "+----------+----------+--------+----------+\n",
      "|payment_id|   date_id|rider_id|pay_amount|\n",
      "+----------+----------+--------+----------+\n",
      "|    539256|2020-08-01|   21826|         9|\n",
      "|    539257|2020-09-01|   21826|         9|\n",
      "|    539258|2020-10-01|   21826|         9|\n",
      "+----------+----------+--------+----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##### Create GOLD Dimension Dataframe #####\n",
    "# Building Gold payment_facts table\n",
    "\n",
    "gold_payment_facts_df = spark.sql(\"\"\"\n",
    "SELECT \n",
    "\tp.payment_id,\n",
    "\tp.date AS date_id,\n",
    "\tp.rider_id,\n",
    "\tp.amount AS pay_amount\n",
    "FROM default.silver_payments_tbl AS p\n",
    "\"\"\")\n",
    "\n",
    "gold_payment_facts_df.printSchema()\n",
    "gold_payment_facts_df.show(5)\n",
    "\n",
    "##### Create GOLD Dimension Table #####\n",
    "gold_payment_facts_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"default.gold_payment_facts_tbl\")\n",
    "\n",
    "spark.read.table(\"gold_payment_facts_tbl\").printSchema()\n",
    "spark.read.table(\"gold_payment_facts_tbl\").show(3)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 3992364160620198,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Louis Notebook - Ingest Data.ipynb FINAL",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
